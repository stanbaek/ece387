
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>🔬 Lab10: CV &#8212; Introduction to Robotic Systems</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Labs/Lab10_CV';</script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="🔬 Lab11: AprilTag" href="Lab11_AprilTag.html" />
    <link rel="prev" title="🔬 Lab9: SLAM" href="Lab9_SLAM.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/ece387.png" class="logo__image only-light" alt="Introduction to Robotic Systems - Home"/>
    <img src="../_static/ece387.png" class="logo__image only-dark pst-js-only" alt="Introduction to Robotic Systems - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    ECE 387 Introduction to Robotic Systems
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course Info</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus.html">📌 Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../schedule.html">📆 Course Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PythonBasic.html">🐍 Python Basic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PythonIntermediate.html">🐍 Python Intermediate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NumPy.html">➕ NumPy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">🙋 FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://stanbaek.github.io/NoPointer.html">No Pointers in Python?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Labs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Lab1_Linux.html">🔬 Lab1: Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab2_ROS.html">🔬 Lab2: ROS</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab3_Python.html">🔬 Lab3: Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab4_Gamepad.html">🔬 Lab4: Gamepad</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab5_TurtleBot.html">🔬 Lab5: Driving the Robot</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab6_IMU.html">🔬 Lab6: IMU-Based Navigation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab7_TF.html">🔬 Lab7: Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab8_LIDAR.html">🔬 Lab 8: LiDAR</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab9_SLAM.html">🔬 Lab9: SLAM</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">🔬 Lab10: CV</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lab11_AprilTag.html">🔬 Lab11: AprilTag</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Appendix/RobotSetup.html">🔧 Robot Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Appendix/MasterSetup.html">🔧 Master Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Appendix/GitSetup.html">🔧 Git Repo Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Appendix/GamepadSetup.html">🔧 Gamepad Setup</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/stanbaek/ece387" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/stanbaek/ece387/issues/new?title=Issue%20on%20page%20%2FLabs/Lab10_CV.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Labs/Lab10_CV.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>🔬 Lab10: CV</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">📌 Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">📜 Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-lab-ros2-client-libraries">🌱 Pre-Lab: ROS2 Client Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-instructions-object-detection-using-hog-and-ros">🧪 Lab Instructions: Object Detection Using HOG and ROS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-a-detector-using-hog-features"><strong>1. Build a Detector Using HOG Features</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-the-detector"><strong>2. Test the Detector</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ros-live-camera-streaming"><strong>3. ROS: Live Camera Streaming</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#capture-training-images-with-ros"><strong>4. Capture Training Images with ROS</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-the-detector-in-a-ros-node"><strong>5. Use the Detector in a ROS Node</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-distance-to-a-stop-sign"><strong>6. Estimating Distance to a Stop Sign</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deliverables">🚚 Deliverables</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lab10-cv">
<h1>🔬 Lab10: CV<a class="headerlink" href="#lab10-cv" title="Link to this heading">#</a></h1>
<section id="objectives">
<h2>📌 Objectives<a class="headerlink" href="#objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Students should be able to explain how Histogram of Oriented Gradients (HOG) features enable object detection.</p></li>
<li><p>Students should be able to train and test a custom stop sign detector using Dlib and evaluate its accuracy.</p></li>
<li><p>Students should be able to stream and process live camera images in ROS 2 using usb_cam and OpenCV.</p></li>
<li><p>Students should be able to implement a ROS node to detect stop signs in real time and estimate their distance.</p></li>
</ul>
</section>
<section id="overview">
<h2>📜 Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>In this lab, we explore how the Histogram of Oriented Gradients (HOG) features, combined with a Support Vector Machine (SVM), enable object detection. By now, we’re all familiar with histograms, but in this context, they help simplify an image so a computer can quickly and accurately identify objects within it.</p>
<p>Rather than analyzing the gradient direction of every single pixel, HOG groups pixels into small cells. Within each cell, the gradient directions are computed and categorized into orientation bins, with stronger gradients carrying more weight. This approach helps reduce the influence of random noise and provides a structured representation of the image. HOG features maintain the distinct shape of an object while allowing for slight variations. For instance, consider an object detector designed to recognize a car:</p>
<a class="reference internal image-reference" href="../_images/Lab10_HOG_Features.jpg"><img alt="../_images/Lab10_HOG_Features.jpg" class="align-center" src="../_images/Lab10_HOG_Features.jpg" style="width: 600px;" /></a>
<br>
<p>Comparing each individual pixel of this training image with another test image would not only be time consuming, but it would also be highly subject to noise.  As previously mentioned, the HOG feature will consider a block of pixels.  The size of this block is variable and will naturally impact both accuracy and speed of execution for the algorithm.  Once the block size is determined, the gradient for each pixel within the block is computed.  Once the gradients are computed for a block, the entire cell can then be represented by this histogram.  Not only does this reduce the amount of data to compare with a test image, but it also reduces the impacts of noise in the image and measurements.</p>
<a class="reference internal image-reference" href="../_images/Lab10_HOG_Histogram.jpg"><img alt="../_images/Lab10_HOG_Histogram.jpg" class="align-center" src="../_images/Lab10_HOG_Histogram.jpg" style="width: 500px;" /></a>
<br>
<p>Now that we understand HOG features, let’s leverage OpenCV and Dlib to build a stop sign detector. First, we need to download a repository containing pre-made test and training data. Keep in mind that we won’t evaluate the algorithm’s effectiveness using the training data—it’s expected to perform well there. Instead, our goal is to use a diverse test set to develop a detector robust enough to recognize stop signs in new images.</p>
</section>
<section id="pre-lab-ros2-client-libraries">
<h2>🌱 Pre-Lab: ROS2 Client Libraries<a class="headerlink" href="#pre-lab-ros2-client-libraries" title="Link to this heading">#</a></h2>
<ol class="arabic">
<li><p>Create a package named <code class="docutils literal notranslate"><span class="pre">lab10_cv</span></code> with the <code class="docutils literal notranslate"><span class="pre">BSD-3-Clause</span></code> license and dependencies:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">rclpy</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cv_bridge</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sensor_msgs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">std_msgs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">opencv2</span></code></p></li>
</ul>
<p><em>Hint: There’s a way to include all dependencies at the time of package creation.</em></p>
</li>
<li><p>Download the <a class="reference download internal" download="" href="../_downloads/6b53939f3cd3f401ea91be886258820e/lab10_prelab.tar.xz"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">lab10_prelab.tar.xz</span></code></span></a>. Extract the files and move them to <code class="docutils literal notranslate"><span class="pre">~/master_ws/src/ece387_ws/lab10_cv/test</span></code>.</p></li>
<li><p>Open the Jupyter Notebook file, <code class="docutils literal notranslate"><span class="pre">lab10_prelab.ipynb</span></code> with vscode.</p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Select</span> <span class="pre">Kernel</span></code> in the top right corner, choose <code class="docutils literal notranslate"><span class="pre">Python</span> <span class="pre">Environments</span></code>, and select <code class="docutils literal notranslate"><span class="pre">/usr/bin/python3</span></code>.</p></li>
<li><p>As you read through the notebook, run the python code by clicking the arrow button in the top left corner.</p></li>
<li><p>If the following message pops up, choose <code class="docutils literal notranslate"><span class="pre">Install</span></code></p>
<a class="reference internal image-reference" href="../_images/Lab10_ipykernel.png"><img alt="../_images/Lab10_ipykernel.png" class="align-center" src="../_images/Lab10_ipykernel.png" style="width: 450px;" /></a>
 <br>
</li>
<li><p>Take a screenshot of the gradient image and submit it on Gradescope.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="lab-instructions-object-detection-using-hog-and-ros">
<h2>🧪 Lab Instructions: Object Detection Using HOG and ROS<a class="headerlink" href="#lab-instructions-object-detection-using-hog-and-ros" title="Link to this heading">#</a></h2>
<p>In this lab, you’ll build a stop sign detector using HOG (Histogram of Oriented Gradients) features, test it, and integrate it with a ROS-based camera system.</p>
<section id="build-a-detector-using-hog-features">
<span id="cv-hog"></span><h3><strong>1. Build a Detector Using HOG Features</strong><a class="headerlink" href="#build-a-detector-using-hog-features" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>Open a terminal and download the demo repository:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>~/master_ws/src
git<span class="w"> </span>clone<span class="w"> </span>git@github.com:ECE495/HOG_Demo.git
<span class="nb">cd</span><span class="w"> </span>HOG_Demo
</pre></div>
</div>
</li>
<li><p>Inside the repo, you’ll find folders for training and test images. We’ll use a tool called <code class="docutils literal notranslate"><span class="pre">imglab</span></code> to annotate the training images.</p></li>
<li><p>Go to <a class="reference external" href="https://solothought.com/imglab/#">imglab</a> in your browser.</p></li>
<li><p>When prompted, click <strong>“UMM, MAYBE NEXT TIME!”</strong> to skip the sign-in.</p></li>
<li><p>In the bottom left corner, click <code class="docutils literal notranslate"><span class="pre">load</span></code>, select the <code class="docutils literal notranslate"><span class="pre">training</span></code> folder from your local files, and then click <code class="docutils literal notranslate"><span class="pre">upload</span></code>. This should load 19 images.</p>
<a class="reference internal image-reference" href="../_images/Lab10_Load.png"><img alt="../_images/Lab10_Load.png" class="align-center" src="../_images/Lab10_Load.png" style="width: 150px;" /></a>
</li>
<li><p>Select the <strong>Rectangle</strong> tool and begin labeling stop signs:</p>
<a class="reference internal image-reference" href="../_images/Lab10_Rectangle.png"><img alt="../_images/Lab10_Rectangle.png" class="align-center" src="../_images/Lab10_Rectangle.png" style="width: 50px;" /></a>
 <br>
<ul class="simple">
<li><p>Click and drag to draw a bounding box <strong>only around each stop sign</strong>.</p></li>
<li><p>If an image contains multiple stop signs, draw a box around each.</p></li>
<li><p>If you make a mistake, select the box and press <code class="docutils literal notranslate"><span class="pre">delete</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is important to label all examples of objects in an image; otherwise, Dlib will implicitly assume that regions not labeled are regions that should not be detected (i.e., hard-negative mining applied during extraction time).</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">Alt</span> <span class="pre">+</span> <span class="pre">←/→</span></code> to switch between images.</p>
</div>
</li>
<li><p>Once you’ve labeled all images, press <code class="docutils literal notranslate"><span class="pre">Ctrl</span> <span class="pre">+</span> <span class="pre">e</span></code> to export your annotations.</p>
<ul class="simple">
<li><p>Save the file as <code class="docutils literal notranslate"><span class="pre">stop_annotations.xml</span></code> in the <code class="docutils literal notranslate"><span class="pre">training</span></code> folder.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/Lab10_Dlib.png"><img alt="../_images/Lab10_Dlib.png" class="align-center" src="../_images/Lab10_Dlib.png" style="width: 200px;" /></a>
</li>
<li><p>Create a Python script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>~/master_ws/src/HOG_Demo
touch<span class="w"> </span>trainDetector.py
</pre></div>
</div>
</li>
<li><p>Open it in your preferred code editor and paste in the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import required libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>  <span class="c1"># For parsing command-line arguments</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dlib</span>      <span class="c1"># For training and testing the object detector</span>

<span class="c1"># Set up command-line arguments</span>
<span class="n">ap</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">ap</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-x&quot;</span><span class="p">,</span> <span class="s2">&quot;--xml&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Path to input XML file&quot;</span><span class="p">)</span>          <span class="c1"># Path to the labeled training dataset in XML format</span>
<span class="n">ap</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-d&quot;</span><span class="p">,</span> <span class="s2">&quot;--detector&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Path to output detector (.svm)&quot;</span><span class="p">)</span>  <span class="c1"># Path where the trained .svm model will be saved</span>
<span class="n">args</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">ap</span><span class="o">.</span><span class="n">parse_args</span><span class="p">())</span>  <span class="c1"># Parse the arguments into a dictionary</span>

<span class="c1"># Inform the user that training is starting</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO] Training detector...&quot;</span><span class="p">)</span>

<span class="c1"># Set training options for the object detector</span>
<span class="n">options</span> <span class="o">=</span> <span class="n">dlib</span><span class="o">.</span><span class="n">simple_object_detector_training_options</span><span class="p">()</span>
<span class="n">options</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="mf">1.0</span>               <span class="c1"># Regularization parameter; higher values = lower bias, higher variance</span>
<span class="n">options</span><span class="o">.</span><span class="n">num_threads</span> <span class="o">=</span> <span class="mi">4</span>      <span class="c1"># Number of CPU threads to use for training</span>
<span class="n">options</span><span class="o">.</span><span class="n">be_verbose</span> <span class="o">=</span> <span class="kc">True</span>    <span class="c1"># Print progress and training status to the console</span>

<span class="c1"># Train the detector using the specified XML file and save the model</span>
<span class="n">dlib</span><span class="o">.</span><span class="n">train_simple_object_detector</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;xml&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;detector&quot;</span><span class="p">],</span> <span class="n">options</span><span class="p">)</span>

<span class="c1"># After training, test the detector on the training dataset to evaluate performance</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO] Training accuracy: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">dlib</span><span class="o">.</span><span class="n">test_simple_object_detector</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;xml&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;detector&quot;</span><span class="p">])))</span>

<span class="c1"># Load the trained detector from disk</span>
<span class="n">detector</span> <span class="o">=</span> <span class="n">dlib</span><span class="o">.</span><span class="n">simple_object_detector</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;detector&quot;</span><span class="p">])</span>

<span class="c1"># Create a window to visualize the learned detector&#39;s HOG features</span>
<span class="n">win</span> <span class="o">=</span> <span class="n">dlib</span><span class="o">.</span><span class="n">image_window</span><span class="p">()</span>
<span class="n">win</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="n">detector</span><span class="p">)</span>  <span class="c1"># Show the detector as a HOG filter visualization</span>

<span class="c1"># Wait for the user to hit Enter before closing</span>
<span class="n">dlib</span><span class="o">.</span><span class="n">hit_enter_to_continue</span><span class="p">()</span>

</pre></div>
</div>
</li>
<li><p>Run the script to train your detector:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>trainDetector.py<span class="w"> </span>--xml<span class="w"> </span>training/stop_annotations.xml<span class="w"> </span>--detector<span class="w"> </span>training/stop_detector.svm
</pre></div>
</div>
<p>You may get a few errors pop up during execution based on your choice for bounding boxes.  Make sure you address those errors before continuing. If everything runs correctly, you’ll see a visualization of the trained HOG filter as shown below. If you get an error, double-check your annotations and fix any issues.</p>
<a class="reference internal image-reference" href="../_images/Lab10_HOG_Visualization.png"><img alt="../_images/Lab10_HOG_Visualization.png" class="align-center" src="../_images/Lab10_HOG_Visualization.png" style="width: 250px;" /></a>
</li>
</ol>
<hr class="docutils" />
</section>
<section id="test-the-detector">
<span id="cv-detector"></span><h3><strong>2. Test the Detector</strong><a class="headerlink" href="#test-the-detector" title="Link to this heading">#</a></h3>
<p>Now it is time to build our code to test the detector.</p>
<ol class="arabic">
<li><p>Create a new Python script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>~/master_ws/src/HOG_Demo
touch<span class="w"> </span>testDetector.py
</pre></div>
</div>
</li>
<li><p>Add the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the necessary packages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">imutils</span><span class="w"> </span><span class="kn">import</span> <span class="n">paths</span>   <span class="c1"># Utility to easily grab file paths from a directory</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>             <span class="c1"># Used to handle command-line arguments</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dlib</span>                 <span class="c1"># Library for machine learning tools including object detection</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span>                  <span class="c1"># OpenCV for image processing</span>

<span class="c1"># Set up command-line arguments</span>
<span class="n">ap</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">ap</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-d&quot;</span><span class="p">,</span> <span class="s2">&quot;--detector&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Path to trained detector (.svm)&quot;</span><span class="p">)</span>
<span class="n">ap</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-t&quot;</span><span class="p">,</span> <span class="s2">&quot;--testing&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Path to testing images folder&quot;</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">ap</span><span class="o">.</span><span class="n">parse_args</span><span class="p">())</span>  <span class="c1"># Parse the arguments into a dictionary</span>

<span class="c1"># Load the trained detector using the path provided</span>
<span class="c1"># The .svm file contains the learned HOG + SVM model</span>
<span class="n">detector</span> <span class="o">=</span> <span class="n">dlib</span><span class="o">.</span><span class="n">simple_object_detector</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;detector&quot;</span><span class="p">])</span>

<span class="c1"># Loop through each image file in the testing directory</span>
<span class="k">for</span> <span class="n">imagePath</span> <span class="ow">in</span> <span class="n">paths</span><span class="o">.</span><span class="n">list_images</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;testing&quot;</span><span class="p">]):</span>

    <span class="c1"># Read the image from disk using OpenCV</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">imagePath</span><span class="p">)</span>

    <span class="c1"># Convert the image from BGR (OpenCV default) to RGB (required by dlib)</span>
    <span class="c1"># Then, pass it to the detector which returns a list of bounding boxes</span>
    <span class="n">boxes</span> <span class="o">=</span> <span class="n">detector</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">))</span>

    <span class="c1"># Loop through each bounding box returned by the detector</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">boxes</span><span class="p">:</span>
        <span class="c1"># Get the coordinates of the bounding box</span>
        <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">left</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">top</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">right</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">bottom</span><span class="p">())</span>

        <span class="c1"># Draw the bounding box on the image using a green rectangle</span>
        <span class="c1"># (x, y) is the top-left, (w, h) is the bottom-right corner</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Display the image with the detections in a pop-up window</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s2">&quot;Detection&quot;</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>

    <span class="c1"># Wait for a key press before moving to the next image</span>
    <span class="c1"># (0 means wait indefinitely until a key is pressed)</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Run your test:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>testDetector.py<span class="w"> </span>--detector<span class="w"> </span>training/stop_detector.svm<span class="w"> </span>--testing<span class="w"> </span><span class="nb">test</span>
</pre></div>
</div>
</li>
</ol>
<p>Look at your results. Did the detector work well? Were there any false positives or missed signs?</p>
</section>
<hr class="docutils" />
<section id="ros-live-camera-streaming">
<h3><strong>3. ROS: Live Camera Streaming</strong><a class="headerlink" href="#ros-live-camera-streaming" title="Link to this heading">#</a></h3>
<p>ROS includes several tools for working with commercial off-the-shelf cameras, like the USB camera on your robot. The main one we’ll use is the <a class="reference external" href="https://docs.ros.org/en/humble/p/usb_cam/">usb_cam</a> package, which is already installed. We’ll now use ROS 2 and <code class="docutils literal notranslate"><span class="pre">usb_cam</span></code> to stream live video from the camera.</p>
<ol class="arabic">
<li><p>SSH into your robot and run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>run<span class="w"> </span>usb_cam<span class="w"> </span>usb_cam_node_exe<span class="w"> </span>--ros-args<span class="w"> </span>-p<span class="w"> </span>video_device:<span class="o">=</span>/dev/video0
</pre></div>
</div>
<p>If you have an error that can’t find the <code class="docutils literal notranslate"><span class="pre">usb_cam</span></code> package, install the package using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>ros-humble-usb-cam
</pre></div>
</div>
<p>If you have a permission error, you need to run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>usermod<span class="w"> </span>-aG<span class="w"> </span>video<span class="w"> </span><span class="nv">$USER</span>
sudo<span class="w"> </span>reboot<span class="w"> </span>now
</pre></div>
</div>
</li>
</ol>
<p>Ignore any calibration error messages — we’ll handle that later.</p>
<ol class="arabic">
<li><p>On your Master PC, Run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>topic<span class="w"> </span>list
</pre></div>
</div>
<p>You should see topics like <code class="docutils literal notranslate"><span class="pre">/image_raw</span></code>, <code class="docutils literal notranslate"><span class="pre">/camera_info</span></code>, etc.</p>
</li>
<li><p>Check topic bandwidth:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>topic<span class="w"> </span>bw<span class="w"> </span>/image_raw
ros2<span class="w"> </span>topic<span class="w"> </span>bw<span class="w"> </span>/image_raw/compressed
</pre></div>
</div>
</li>
<li><p>Check image publishing rate:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>topic<span class="w"> </span>hz<span class="w"> </span>/image_raw
ros2<span class="w"> </span>topic<span class="w"> </span>bw<span class="w"> </span>/image_raw/compressed
</pre></div>
</div>
</li>
<li><p>View the camera feed:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>run<span class="w"> </span>rqt_image_view<span class="w"> </span>rqt_image_view
</pre></div>
</div>
<p>Select <code class="docutils literal notranslate"><span class="pre">/image_raw</span></code> to see the feed. Try waving your hand in front of the camera to check latency.</p>
</li>
<li><p>You may have noticed that streaming images over WiFi is quite slow. While using compressed images can help, it also increases the processing load on both the Raspberry Pi and the master computer. To avoid this during development, we’ll run camera-related code directly on the master computer. Once everything is working, we can move the code back to the robot.</p></li>
<li><p>Disconnect the camera from the robot and plug it into the master computer. You can also unplug the gamepad—it’s not needed for now.</p></li>
<li><p>On the master computer, run the following command to start the camera node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>run<span class="w"> </span>usb_cam<span class="w"> </span>usb_cam_node_exe<span class="w"> </span>--ros-args<span class="w"> </span>-p<span class="w"> </span>video_device:<span class="o">=</span>/dev/video0
</pre></div>
</div>
</li>
<li><p>Check topic bandwidth and image publishing rate:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>topic<span class="w"> </span>bw<span class="w"> </span>/image_raw
ros2<span class="w"> </span>topic<span class="w"> </span>hz<span class="w"> </span>/image_raw
</pre></div>
</div>
</li>
<li><p>To verify the image stream, launch the image viewer:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>run<span class="w"> </span>rqt_image_view<span class="w"> </span>rqt_image_view
</pre></div>
</div>
</li>
</ol>
</section>
<section id="capture-training-images-with-ros">
<h3><strong>4. Capture Training Images with ROS</strong><a class="headerlink" href="#capture-training-images-with-ros" title="Link to this heading">#</a></h3>
<p>You’ll now use a script to save training images of stop signs from your live feed.</p>
<ol class="arabic">
<li><p>Download the <a class="reference download internal" download="" href="../_downloads/2a6b0742e5e4a2f607ed3ac0bfd5cf97/image_capture.py"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">image_capture.py</span></code></span></a> script and place it in your package’s script folder.</p></li>
<li><p>Update your <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> to include the script as an entry point. This is necessary to ensure that the script runs as a node.</p></li>
<li><p>Open the <code class="docutils literal notranslate"><span class="pre">image_capture.py</span></code> script and read through the code carefully. It may be unfamiliar, but take your time to understand what each part is doing.</p></li>
<li><p>Rebuild your package:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ccbuild<span class="w"> </span>--packages-selelct<span class="w"> </span>labl0-cv
</pre></div>
</div>
</li>
<li><p>Run the USB camera node on Master:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>run<span class="w"> </span>usb_cam<span class="w"> </span>usb_cam_node_exe<span class="w"> </span>--ros-args<span class="w"> </span>-p<span class="w"> </span>video_device:<span class="o">=</span>/dev/video0
</pre></div>
</div>
</li>
<li><p>Open a new terminal, navigate to the <code class="docutils literal notranslate"><span class="pre">Documents</span></code> directory, and run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>run<span class="w"> </span>lab10_cv<span class="w"> </span>image_capture<span class="w"> </span>-o<span class="w"> </span>./training_images/
</pre></div>
</div>
<p>Press <code class="docutils literal notranslate"><span class="pre">s</span></code> and <code class="docutils literal notranslate"><span class="pre">Enter</span></code> to save an image. Take multiple images from different angles and distances. Press <code class="docutils literal notranslate"><span class="pre">Ctrl</span> <span class="pre">+</span> <span class="pre">C</span></code> when done.</p>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">imglab</span></code> again to annotate your new images and save the XML file as before. Then re-train your detector using the updated dataset.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="use-the-detector-in-a-ros-node">
<h3><strong>5. Use the Detector in a ROS Node</strong><a class="headerlink" href="#use-the-detector-in-a-ros-node" title="Link to this heading">#</a></h3>
<p>In this step, you’ll create a ROS 2 node that runs your stop sign detector in real-time using the live camera feed.</p>
<ol class="arabic">
<li><p>Download the <a class="reference download internal" download="" href="../_downloads/bb1594c198f2d70ea7a93073ec85839c/stop_detector.py"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">stop_detector.py</span></code></span></a> script and place it in your package’s script folder.</p></li>
<li><p>Update your <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> to include the script as an entry point.</p></li>
<li><p>Edit <code class="docutils literal notranslate"><span class="pre">stop_detector.py</span></code>.</p>
<ul class="simple">
<li><p>Inside the <code class="docutils literal notranslate"><span class="pre">camera_callback()</span></code> function, use the same image processing approach used in the <code class="docutils literal notranslate"><span class="pre">image_capture.py</span></code> script to convert ROS image messages into OpenCV format (<code class="docutils literal notranslate"><span class="pre">cv_image</span></code>).</p></li>
<li><p>Apply your HOG-based detector to the <code class="docutils literal notranslate"><span class="pre">cv_image</span></code>, just like you did in <a class="reference internal" href="#cv-detector"><span class="std std-ref">Test the Detector</span></a>.</p></li>
<li><p>Draw bounding boxes around any detected stop signs. Use the <code class="docutils literal notranslate"><span class="pre">cv2.rectangle</span></code> function to draw the boxes.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">cv2.imshow()</span></code> to display the video, and make sure to call <code class="docutils literal notranslate"><span class="pre">cv2.waitKey(1)</span></code> to allow the video to refresh in real-time.</p></li>
</ul>
</li>
<li><p>Use the following command, replacing <code class="docutils literal notranslate"><span class="pre">&lt;path/to/detector&gt;</span></code> with the path to your trained <code class="docutils literal notranslate"><span class="pre">.svm</span></code> detector file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>run<span class="w"> </span>lab10_cv<span class="w"> </span>stop_detector<span class="w"> </span>-d<span class="w"> </span>&lt;path/to/detector&gt;
</pre></div>
</div>
<p>You should now see a window displaying the camera feed with stop signs outlined.</p>
</li>
<li><p>Install <code class="docutils literal notranslate"><span class="pre">OBS</span> <span class="pre">Studio</span></code> to record your stop sign detection in action.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>update
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>obs-studio<span class="w"> </span>qtwayland5
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">obs</span></code> from the terminal, or find it under <code class="docutils literal notranslate"><span class="pre">Applications</span></code>.</p></li>
<li><p>Set up recording:</p>
<ul class="simple">
<li><p>In the <em>Scenes</em> panel (bottom-left), click the <strong><code class="docutils literal notranslate"><span class="pre">+</span></code></strong> and name your scene (e.g., “Stop Sign Detection”).</p></li>
<li><p>In the <em>Sources</em> panel, click the <strong><code class="docutils literal notranslate"><span class="pre">+</span></code></strong> and choose <strong>Window Capture</strong>.</p></li>
<li><p>Select the window showing your detector output.</p></li>
<li><p>Click <strong>Start Recording</strong>.</p></li>
</ul>
</li>
<li><p>Move around while keeping the stop sign visible to showcase detection capabilities. Present the stop sign from different angles, including varied pan, tilt, and rotation positions, as demonstrated in the video below.</p></li>
<li><p>When finished, click <strong>Stop Recording</strong>.</p></li>
<li><p>Upload your recording to the class Teams channel.</p></li>
</ol>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/9eR6FfNGUfo?si=rEm6W9HU0e8J9pK6" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</center>
</section>
<section id="estimating-distance-to-a-stop-sign">
<h3><strong>6. Estimating Distance to a Stop Sign</strong><a class="headerlink" href="#estimating-distance-to-a-stop-sign" title="Link to this heading">#</a></h3>
<p>In this section, you’ll determine the distance between the stop sign and the camera using its known size and a detector.</p>
<ol class="arabic">
<li><p><strong>Calculate the Camera’s Focal Length</strong>: Start with a stop sign that has a known width, <span class="math notranslate nohighlight">\(Y\)</span>, positioned at a known distance, <span class="math notranslate nohighlight">\(Z\)</span>, from the camera. The detector identifies the stop sign and provides its perceived width in pixels, <span class="math notranslate nohighlight">\(y\)</span>. Using these values, calculate the camera’s focal length, <span class="math notranslate nohighlight">\(f\)</span>, with the following formula:</p>
<div class="math notranslate nohighlight">
\[f = Z \times \frac{y}{Y}\]</div>
<p>To determine the focal length, print the perceived width of the stop sign, <span class="math notranslate nohighlight">\(y\)</span>.</p>
<a class="reference internal image-reference" href="../_images/Lab10_CameraModel.png"><img alt="../_images/Lab10_CameraModel.png" class="align-center" src="../_images/Lab10_CameraModel.png" style="width: 300px;" /></a>
 <br>
</li>
<li><p><strong>Estimate the Distance to the Stop Sign</strong>: Once you have the calculated focal length, <span class="math notranslate nohighlight">\(f\)</span>, use it along with the known width of the stop sign, <span class="math notranslate nohighlight">\(Y\)</span>, and its perceived width in pixels, <span class="math notranslate nohighlight">\(y\)</span>, to compute the distance from the camera using this formula:</p>
<div class="math notranslate nohighlight">
\[Z = f \times \frac{Y}{y}\]</div>
</li>
<li><p><strong>Implement a Class for Distance Calculation</strong></p>
<ul class="simple">
<li><p>Define two class variables:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">FOCAL</span></code>: The calculated focal length.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">STOP_WIDTH</span></code>: The known width of the stop sign.</p></li>
</ul>
</li>
<li><p>Create a class function named <code class="docutils literal notranslate"><span class="pre">get_distance</span></code>, which calculates the distance using the <code class="docutils literal notranslate"><span class="pre">FOCAL</span></code> length and <code class="docutils literal notranslate"><span class="pre">STOP_WIDTH</span></code>.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Pay attention to the <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">w</span></code> variables in the <code class="docutils literal notranslate"><span class="pre">box</span></code>—understanding their role is crucial!</p>
</div>
</li>
<li><p><strong>Publish the Distance</strong></p>
<ul class="simple">
<li><p>Set up a publisher to send the calculated distance using <strong>Float32</strong> messages from the <em>std_msgs</em> package on the <em>/stop_dist</em> topic.</p></li>
<li><p>Ensure the published distance reflects every detected object in the image.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="deliverables">
<h2>🚚 Deliverables<a class="headerlink" href="#deliverables" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>[15 Points] Complete the <code class="docutils literal notranslate"><span class="pre">stop_detector.py</span></code> Script</strong></p>
<ul class="simple">
<li><p>Ensure the script is fully functional and implements all required features.</p></li>
<li><p>Push your code to GitHub and confirm that it has been successfully uploaded.
<strong>NOTE:</strong> <em>If the instructor can’t find your code in your repository, you will receive a grade of 0 for the coding part.</em></p></li>
</ul>
</li>
<li><p><strong>[10 Points] Submit Screenshots</strong></p>
<ul class="simple">
<li><p>Submit the screenshot of the gradient image from the pre-lab Jupyter notebook.</p></li>
</ul>
</li>
<li><p><strong>[15 Points] Demonstration</strong></p>
<ul class="simple">
<li><p>Show the <code class="docutils literal notranslate"><span class="pre">stop_detector</span></code> node successfully detect the stop sign.</p></li>
<li><p>OBS recording showing real-time stop sign detection (upload to Teams).</p></li>
<li><p>Must include varied angles/distances and display bounding boxes.</p></li>
<li><p>Published distances on /stop_dist (verified via ros2 topic echo).</p></li>
</ul>
</li>
<li><p><strong>[10 points] Summary</strong></p>
<ul class="simple">
<li><p>Brief summary of HOG principles, challenges faced, and detection accuracy.</p></li>
</ul>
</li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Labs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Lab9_SLAM.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">🔬 Lab9: SLAM</p>
      </div>
    </a>
    <a class="right-next"
       href="Lab11_AprilTag.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">🔬 Lab11: AprilTag</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">📌 Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">📜 Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-lab-ros2-client-libraries">🌱 Pre-Lab: ROS2 Client Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-instructions-object-detection-using-hog-and-ros">🧪 Lab Instructions: Object Detection Using HOG and ROS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-a-detector-using-hog-features"><strong>1. Build a Detector Using HOG Features</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-the-detector"><strong>2. Test the Detector</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ros-live-camera-streaming"><strong>3. ROS: Live Camera Streaming</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#capture-training-images-with-ros"><strong>4. Capture Training Images with ROS</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-the-detector-in-a-ros-node"><strong>5. Use the Detector in a ROS Node</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-distance-to-a-stop-sign"><strong>6. Estimating Distance to a Stop Sign</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deliverables">🚚 Deliverables</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Stan Baek
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>